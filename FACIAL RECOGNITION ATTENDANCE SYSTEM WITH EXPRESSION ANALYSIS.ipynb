{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "L_cQX8dWu4Dv"
      },
      "source": [
        "## Facial Recognition Attendance System with Expression Analysis\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "O6PN9FvIx614"
      },
      "source": [
        "## Installation of Packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxbHBsF-8Y_l"
      },
      "outputs": [],
      "source": [
        "pip install mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install deepface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "import matplotlib.pyplot as plt\n",
        "from deepface import DeepFace\n",
        "import csv\n",
        "import datetime\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downloading TensorFlow Lite Model (BlazeFace) using 'requests' library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "url = 'https://storage.googleapis.com/mediapipe-models/face_detector/blaze_face_short_range/float16/1/blaze_face_short_range.tflite'\n",
        "filename = 'detector.tflite'\n",
        "\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    print('Download completed.')\n",
        "else:\n",
        "    print('Failed to download the file.')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "89BlskiiyGDC"
      },
      "source": [
        "## Visualization utilities"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wLHhoIkkWYLQ"
      },
      "source": [
        "To better demonstrate the Face Detector API, created a set of visualization tools that will be used in this colab. These will draw a bounding box around detected faces, as well as markers over certain detected points on the faces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4aPO-hvbw3r"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, Union\n",
        "import math\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "MARGIN = 10  # pixels\n",
        "ROW_SIZE = 10  # pixels\n",
        "FONT_SIZE = 1\n",
        "FONT_THICKNESS = 1\n",
        "TEXT_COLOR = (255, 0, 0)  # red\n",
        "\n",
        "\n",
        "def _normalized_to_pixel_coordinates(\n",
        "    normalized_x: float, normalized_y: float, image_width: int,\n",
        "    image_height: int) -> Union[None, Tuple[int, int]]:\n",
        "  \"\"\"Converts normalized value pair to pixel coordinates.\"\"\"\n",
        "\n",
        "  # Checks if the float value is between 0 and 1.\n",
        "  def is_valid_normalized_value(value: float) -> bool:\n",
        "    return (value > 0 or math.isclose(0, value)) and (value < 1 or\n",
        "                                                      math.isclose(1, value))\n",
        "\n",
        "  if not (is_valid_normalized_value(normalized_x) and\n",
        "          is_valid_normalized_value(normalized_y)):\n",
        "    # TODO: Draw coordinates even if it's outside of the image bounds.\n",
        "    return None\n",
        "  x_px = min(math.floor(normalized_x * image_width), image_width - 1)\n",
        "  y_px = min(math.floor(normalized_y * image_height), image_height - 1)\n",
        "  return x_px, y_px\n",
        "\n",
        "\n",
        "def visualize(\n",
        "    image,\n",
        "    detection_result\n",
        ") -> np.ndarray:\n",
        "  \"\"\"Draws bounding boxes and keypoints on the input image and return it.\n",
        "  Args:\n",
        "    image: The input RGB image.\n",
        "    detection_result: The list of all \"Detection\" entities to be visualize.\n",
        "  Returns:\n",
        "    Image with bounding boxes.\n",
        "  \"\"\"\n",
        "  annotated_image = image.copy()\n",
        "  height, width, _ = image.shape\n",
        "\n",
        "  for detection in detection_result.detections:\n",
        "    # Draw bounding_box\n",
        "    bbox = detection.bounding_box\n",
        "    start_point = bbox.origin_x, bbox.origin_y\n",
        "    end_point = bbox.origin_x + bbox.width, bbox.origin_y + bbox.height\n",
        "    cv2.rectangle(annotated_image, start_point, end_point, TEXT_COLOR, 3)\n",
        "\n",
        "    # Draw keypoints\n",
        "    for keypoint in detection.keypoints:\n",
        "      keypoint_px = _normalized_to_pixel_coordinates(keypoint.x, keypoint.y,\n",
        "                                                     width, height)\n",
        "      color, thickness, radius = (0, 255, 0), 2, 2\n",
        "      cv2.circle(annotated_image, keypoint_px, thickness, color, radius)\n",
        "\n",
        "    # Draw label and score\n",
        "    category = detection.categories[0]\n",
        "    category_name = category.category_name\n",
        "    category_name = '' if category_name is None else category_name\n",
        "    probability = round(category.score, 2)\n",
        "    result_text = category_name + ' (' + str(probability) + ')'\n",
        "    text_location = (MARGIN + bbox.origin_x,\n",
        "                     MARGIN + ROW_SIZE + bbox.origin_y)\n",
        "    cv2.putText(annotated_image, result_text, text_location, cv2.FONT_HERSHEY_PLAIN,\n",
        "                FONT_SIZE, TEXT_COLOR, FONT_THICKNESS)\n",
        "\n",
        "  return annotated_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CAPTURING THE IMAGE FROM CAMERA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cap=cv2.VideoCapture(0)\n",
        "while cap.isOpened():\n",
        "    success,frame=cap.read()\n",
        "    if not success:\n",
        "        break\n",
        "    cv2.imshow(\"Capture my Video\",cv2.flip(frame,1))   \n",
        "    if cv2.waitKey(100) & 0xFF == ord('e'):\n",
        "        break\n",
        "flipped_frame = cv2.flip(frame, 1)      \n",
        "plt.imshow(flipped_frame[:, :, ::-1])\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the video capture object to use the webcam\n",
        "cap = cv2.VideoCapture(0)\n",
        "while cap.isOpened():\n",
        "    success,frame=cap.read()\n",
        "    if not success:\n",
        "        break\n",
        "    flipped_frame = cv2.flip(frame, 1)\n",
        "    org = (50, 50)\n",
        "    cv2.putText(flipped_frame, \"Present\", org, cv2.FONT_HERSHEY_SIMPLEX, \n",
        "                fontScale=1.5, color=(255, 0, 0), thickness=3, lineType=cv2.LINE_AA)\n",
        "    cv2.imshow(\"Attendance\",flipped_frame)\n",
        "    \n",
        "    if cv2.waitKey(100) & 0xFF == ord('e'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "OR INSERTING THE DOWNLOADED IMAGE FROM SYSTEM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_path = 'C:\\\\Users\\\\mnkmr\\\\Downloads\\\\SAMPLE FACE DETECT IMAGE.png'  \n",
        "frame = cv2.imread(image_path)\n",
        "if frame is None:\n",
        "    print(\"Failed to load the image\")\n",
        "    exit()\n",
        "cv2.imshow(\" Download Image From System\", frame)# Display the image\n",
        "flipped_frame = frame     \n",
        "plt.imshow(flipped_frame[:, :, ::-1])\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy4r2_ePylIa"
      },
      "source": [
        "## Running inference and visualizing the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "for downloaded image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "\n",
        "# Load the image using OpenCV\n",
        "image_path = 'C:\\\\Users\\\\mnkmr\\\\Downloads\\\\SAMPLE FACE DETECT IMAGE.png'\n",
        "image = cv2.imread(image_path)\n",
        "\n",
        "# Initialize MediaPipe face detection\n",
        "mp_face_detection = mp.solutions.face_detection\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "# Convert the image to RGB format (required by MediaPipe)\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Run face detection on the image\n",
        "with mp_face_detection.FaceDetection(min_detection_confidence=0.5) as face_detection:\n",
        "    results = face_detection.process(image_rgb)\n",
        "    if results.detections:\n",
        "        for detection in results.detections:\n",
        "            bbox = detection.location_data.relative_bounding_box\n",
        "            ih, iw, _ = image.shape\n",
        "            x, y, w, h = int(bbox.xmin * iw), int(bbox.ymin * ih), \\\n",
        "                         int(bbox.width * iw), int(bbox.height * ih)\n",
        "            \n",
        "            # Draw bounding box and landmarks on the image\n",
        "            mp_drawing.draw_detection(image, detection) \n",
        "\n",
        "# Display the image with identification marks\n",
        "cv2.imshow(\"Facial Detection\", image)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "for directly captured image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNJq-ygtZX7J"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "\n",
        "# Create a MediaPipe face detection pipeline\n",
        "mp_face_detection = mp.solutions.face_detection\n",
        "face_detection = mp_face_detection.FaceDetection()\n",
        "\n",
        "# Open a video capture object\n",
        "cap = cv2.VideoCapture(0)  # Use 0 for the default camera\n",
        "\n",
        "while True:\n",
        "    # Read a frame from the video capture\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    # Flip the frame horizontally\n",
        "    frame = cv2.flip(frame, 1)\n",
        "\n",
        "    # Convert the frame to RGB format\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Run face detection\n",
        "    results = face_detection.process(frame_rgb)\n",
        "\n",
        "    # Draw bounding boxes and landmarks on the frame\n",
        "    if results.detections:\n",
        "        for detection in results.detections:\n",
        "            bbox = detection.location_data.relative_bounding_box\n",
        "            h, w, _ = frame.shape\n",
        "            xmin = int(bbox.xmin * w)\n",
        "            ymin = int(bbox.ymin * h)\n",
        "            width = int(bbox.width * w)\n",
        "            height = int(bbox.height * h)\n",
        "            xmax = xmin + width\n",
        "            ymax = ymin + height\n",
        "\n",
        "            # Draw bounding box\n",
        "            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
        "\n",
        "            # Draw landmarks\n",
        "            for keypoint in detection.location_data.relative_keypoints:\n",
        "                x = int(keypoint.x * w)\n",
        "                y = int(keypoint.y * h)\n",
        "                cv2.circle(frame, (x, y), 2, (0, 255, 0), -1)\n",
        "\n",
        "    # Display the annotated frame\n",
        "    cv2.imshow(\"Annotated Frame\", frame)\n",
        "\n",
        "    # Check for key press\n",
        "    if cv2.waitKey(100) & 0xFF == ord('e'):\n",
        "        break\n",
        "# Release the video capture and close the windows\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ATTENDANCE RECOGNITION\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from deepface import DeepFace\n",
        "import csv\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "face_objs = DeepFace.extract_faces(flipped_frame, target_size=(224, 224), \n",
        "                                           detector_backend='opencv', enforce_detection=False,\n",
        "                                           align=True, grayscale=False)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if face_objs:\n",
        "    extracted_face = face_objs[0][\"face\"]\n",
        "    plt.imshow(extracted_face[:, :, ::-1])\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    obj=DeepFace.analyze(flipped_frame,enforce_detection=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(obj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "analysis = DeepFace.analyze(flipped_frame, actions=['emotion'], enforce_detection=False)\n",
        "\n",
        "        # If faces are detected, analysis will be a list\n",
        "if isinstance(analysis, list):\n",
        "    for obj in analysis:\n",
        "       dominant_emotion = obj['dominant_emotion']\n",
        "                \n",
        "                # Display the detected emotion\n",
        "print(f\"Detected emotion: {dominant_emotion}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Open a CSV file to record attendance\n",
        "file = open('attendance.csv', mode='w', newline='')\n",
        "writer = csv.writer(file)\n",
        "writer.writerow(['Name', 'Date', 'Time', 'Expression'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name = \"Student\"  # Replace with actual name recognition logic if available\n",
        "now = datetime.datetime.now()\n",
        "date = now.strftime(\"%Y-%m-%d\")\n",
        "time = now.strftime(\"%H:%M:%S\")\n",
        "writer.writerow([name, date, time, dominant_emotion])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "            \n",
        "# Release the video capture object and close all OpenCV windows\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
